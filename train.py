from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling
from datasets import Dataset
from transformers import Trainer, TrainingArguments
import math, os

# –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –≤—ã–¥–µ–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å
tokenizer = GPT2Tokenizer.from_pretrained("distilgpt2")
tokenizer.add_special_tokens({
    'pad_token': '<|pad|>',
    'additional_special_tokens': ['<|context|>', '<|joke|>']
    })

model = GPT2LMHeadModel.from_pretrained("distilgpt2")
model.gradient_checkpointing_enable()
model.resize_token_embeddings(len(tokenizer))

# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
def load_dataset_from_txt(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —à—É—Ç–∫—É (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    contexts = []
    jokes = []
    for line in lines:
        if "<|context|>" in line and "<|joke|>" in line:
            parts = line.split("<|joke|>")
            contexts.append(parts[0].replace("<|context|>", "").strip())
            jokes.append(parts[1].strip())
    
    return {"context": contexts, "joke": jokes}

data = load_dataset_from_txt("datasets\\jokes_dataset_v4.txt")
dataset = Dataset.from_dict(data)
dataset = dataset.train_test_split(test_size=0.1)
train_data = dataset["train"]
val_data = dataset["test"]

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
def tokenize_function(examples):
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —à—É—Ç–∫—É –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç
    texts = [f"<|context|> {ctx} <|joke|> {joke}" for ctx, joke in zip(examples["context"], examples["joke"])]
    return tokenizer(texts, truncation=True, max_length=96, padding="max_length")

tokenized_train = train_data.map(tokenize_function, batched=True, remove_columns=["context", "joke"])
tokenized_val = val_data.map(tokenize_function, batched=True, remove_columns=["context", "joke"])


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    loss_fct = torch.nn.CrossEntropyLoss()
    loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1)).item()
    return {
        "eval_loss": loss,
        "perplexity": math.exp(loss) if loss < 100 else float("inf"),
    }

# –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    dataloader_num_workers=2,
    fp16=True,
    output_dir="./joke-gpt2",
    overwrite_output_dir=True,
    num_train_epochs=10,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    save_steps=5000,
    save_total_limit=2,
    logging_steps=500,
    report_to="tensorboard",
    logging_dir="./logs",
    optim="adamw_torch",  # –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    compute_metrics=compute_metrics,  
)

if __name__ == '__main__':
    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    import torch
    print("üíª –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU")
    trainer.train()
    model.save_pretrained("trained-joke-distilgpt2_testV4")
    tokenizer.save_pretrained("trained-joke-distilgpt2_testV4")


    # torch.cuda.empty_cache()



    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
    # trainer.args.device = torch.device("cpu")
    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ CPU
    # trainer.model = trainer.model.to("cpu")
    # eval_results = trainer.evaluate()
    # print("Evaluation results:", eval_results)
